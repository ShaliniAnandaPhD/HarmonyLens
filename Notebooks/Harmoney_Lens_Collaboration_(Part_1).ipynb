{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSHcl/QlCBEwFtlSpGGhY9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaliniAnandaPhD/HarmonyLens/blob/main/Harmoney_Lens_Collaboration_(Part_1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs necessary Python packages for the script, including tweepy for Twitter API access, feedparser for parsing RSS feeds, nltk for natural language processing, transformers for sentiment analysis, cachetools for caching API responses, and logging for error and activity logging.\n"
      ],
      "metadata": {
        "id": "rcNlF_Jdb71j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweepy feedparser nltk transformers cachetools logging"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyTh5T13ScOP",
        "outputId": "2555611d-cbf8-4d62-fa85-21594622ab29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.10/dist-packages (4.14.0)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (6.0.11)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (5.3.2)\n",
            "Collecting logging\n",
            "  Using cached logging-0.4.9.6.tar.gz (96 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs necessary Python packages for the script, including tweepy for Twitter API access, feedparser for parsing RSS feeds, nltk for natural language processing, transformers for sentiment analysis, cachetools for caching API responses, and logging for error and activity logging.\n"
      ],
      "metadata": {
        "id": "tCfngqoGcF2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import requests\n",
        "import feedparser\n",
        "import json\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from cachetools import cached, TTLCache\n",
        "import logging\n",
        "\n",
        "# Set up basic logging\n",
        "logging.basicConfig(filename='app.log', level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger()\n"
      ],
      "metadata": {
        "id": "lpqjuBLaSeHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defines a function to call APIs with caching enabled to reduce the number of requests. It includes error handling to manage different types of exceptions that might occur during API calls.\n"
      ],
      "metadata": {
        "id": "bjlv__JtcHdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install feedparser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq4N34vROjXg",
        "outputId": "c196b0cb-5c05-4cdc-dc26-e98c8a731df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (6.0.11)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implements a function to gather data from various APIs including Twitter, Reddit, New York Times, and BBC. It uses the previously defined API call function with error handling and logging.\n"
      ],
      "metadata": {
        "id": "N-DYCuDRcL6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache = TTLCache(maxsize=100, ttl=3600)\n",
        "\n",
        "@cached(cache)\n",
        "def call_api(url, headers=None, params=None):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, params=params)\n",
        "        response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
        "        return response.json()\n",
        "    except requests.exceptions.HTTPError as errh:\n",
        "        logger.error(\"Http Error:\", errh)\n",
        "    except requests.exceptions.ConnectionError as errc:\n",
        "        logger.error(\"Error Connecting:\", errc)\n",
        "    except requests.exceptions.Timeout as errt:\n",
        "        logger.error(\"Timeout Error:\", errt)\n",
        "    except requests.exceptions.RequestException as err:\n",
        "        logger.error(\"OOps: Something Else\", err)\n"
      ],
      "metadata": {
        "id": "LCzsdnXeZrlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modifies the data gathering function to focus specifically on collecting data related to \"peace talks\" and \"peace negotiations\". The data is then saved in 'output2.json'.\n"
      ],
      "metadata": {
        "id": "uRn5fhe2cSsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO keyword search\n",
        "import requests\n",
        "import feedparser\n",
        "import json\n",
        "\n",
        "def gather_data_from_apis():\n",
        "    # API endpoints and credentials\n",
        "    twitter_api = \"https://api.twitter.com/2/tweets/search/recent\"\n",
        "    reddit_api = \"https://www.reddit.com/r/news/.json\"\n",
        "    nytimes_api = \"https://api.nytimes.com/svc/news/v3/content/all/all.json\"\n",
        "    bbc_api = \"http://feeds.bbci.co.uk/news/rss.xml\"\n",
        "\n",
        "    # Twitter API\n",
        "    twitter_headers = {\"Authorization\": \"AAAAAAAAAAAAAAAAAAAAAPGOrgEAAAAAmo4Z%2FY5Gn%2FRq%2FTTthYEU2LMjl9o%3DYxENYJyzjuhNVO1e9futX7vnLVEXsz5TfnmH5bsLccDUJ0p9at\"}\n",
        "    twitter_response = requests.get(twitter_api, headers=twitter_headers)\n",
        "\n",
        "    # Reddit API\n",
        "    #reddit_headers = {\"User-Agent\": \"YourAppName\"}\n",
        "    #reddit_response = requests.get(reddit_api, headers=reddit_headers)\n",
        "\n",
        "    # New York Times API\n",
        "    nytimes_params = {\"api-key\": \"km7meZc5wxnL0KinmSKwTefAIlFLSb6G\"}\n",
        "    nytimes_response = requests.get(nytimes_api, params=nytimes_params)\n",
        "\n",
        "    # BBC News RSS feed\n",
        "    bbc_response = requests.get(bbc_api)\n",
        "\n",
        "    # Parsing and combining data\n",
        "    data = []\n",
        "\n",
        "    if twitter_response.ok:\n",
        "        data.extend(twitter_response.json().get(\"data\", []))\n",
        "\n",
        "    #if reddit_response.ok:\n",
        "        #reddit_data = reddit_response.json()\n",
        "        #data.extend([child['data'] for child in reddit_data['data']['children']])\n",
        "\n",
        "    if nytimes_response.ok:\n",
        "        nytimes_data = nytimes_response.json()\n",
        "        data.extend(nytimes_data.get(\"results\", []))\n",
        "\n",
        "    if bbc_response.ok:\n",
        "        feed = feedparser.parse(bbc_response.content)\n",
        "        for entry in feed.entries:\n",
        "            data.append({\"title\": entry.title, \"link\": entry.link, \"published\": entry.published})\n",
        "\n",
        "    return data\n",
        "\n",
        "def save_data_to_file(data, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump(data, file, indent=4)\n",
        "\n",
        "# Usage\n",
        "data = gather_data_from_apis()\n",
        "save_data_to_file(data, 'output.json')\n"
      ],
      "metadata": {
        "id": "HIiZNJ4_Prvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gather_data_from_apis():\n",
        "    # Define keywords related to peace talks and negotiations\n",
        "    keywords = [\"peace talks\", \"peace negotiations\"]\n",
        "\n",
        "    # Combine keywords into a query for the Twitter API\n",
        "    twitter_query = \" OR \".join(keywords) + \" -filter:retweets\"\n",
        "    twitter_headers = {\"Authorization\": \"AAAAAAAAAAAAAAAAAAAAAPGOrgEAAAAAmo4Z%2FY5Gn%2FRq%2FTTthYEU2LMjl9o%3DYxENYJyzjuhNVO1e9futX7vnLVEXsz5TfnmH5bsLccDUJ0p9at\"}\n",
        "    twitter_api = \"https://api.twitter.com/2/tweets/search/recent?query=\" + twitter_query\n",
        "\n",
        "    # Fetch data from Twitter API\n",
        "    twitter_response = requests.get(twitter_api, headers=twitter_headers)\n",
        "\n",
        "    # Parsing and combining data\n",
        "    data = []\n",
        "    if twitter_response.ok:\n",
        "        data.extend([tweet[\"text\"] for tweet in twitter_response.json().get(\"data\", [])])\n",
        "\n",
        "    # Add more APIs if needed\n",
        "\n",
        "    return data\n",
        "\n",
        "data = gather_data_from_apis()\n",
        "save_data_to_file(data, 'output.json')\n"
      ],
      "metadata": {
        "id": "smsy6WZvVdlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloads necessary NLTK resources and defines a function for preprocessing text data. This function tokenizes the text, removes stopwords and non-alphabetic words, and converts them to lowercase.\n"
      ],
      "metadata": {
        "id": "osx3Jz33cadC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "yb0M0tORTb5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXAKWoRJTdaq",
        "outputId": "1888fbee-3f20-4449-dbf7-caa6bad1db60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loads raw data from 'output.json', then preprocesses this data using the function defined in the previous cell.\n"
      ],
      "metadata": {
        "id": "QZ-MxAlpcfXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text_data(data):\n",
        "    processed_data = []\n",
        "    for item in data:\n",
        "        # Extract text content from each item\n",
        "        # Assuming 'text' key contains the textual content\n",
        "        text = item.get('text', '')\n",
        "\n",
        "        # Tokenize the text\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Remove stopwords and non-alphabetic words, and convert to lowercase\n",
        "        processed_text = [word.lower() for word in tokens if word.isalpha() and word not in stopwords.words('english')]\n",
        "\n",
        "        processed_data.append(processed_text)\n",
        "\n",
        "    return processed_data\n"
      ],
      "metadata": {
        "id": "ZkRDuKpsTf43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E_AYble3ciZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_from_file(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "# Load raw data from 'output.json'\n",
        "raw_data = load_data_from_file('output.json')\n",
        "\n",
        "# Preprocess the loaded data\n",
        "processed_data = preprocess_text_data(raw_data)\n"
      ],
      "metadata": {
        "id": "ekfRiNr1Tkoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loads raw data from 'output.json', then preprocesses this data using the function defined in the previous cell.\n"
      ],
      "metadata": {
        "id": "dBy84-DIckUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n"
      ],
      "metadata": {
        "id": "d3U90zkyTpOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_sentiment_analysis(data):\n",
        "    \"\"\"\n",
        "    Perform sentiment analysis on a list of text strings.\n",
        "\n",
        "    Args:\n",
        "    data (list of str): A list of text strings to analyze.\n",
        "\n",
        "    Returns:\n",
        "    list of dict: A list of dictionaries containing the sentiment analysis results.\n",
        "    \"\"\"\n",
        "    # Check if CUDA is available for faster processing\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "    # Initialize the sentiment analysis model pipeline\n",
        "    # The model automatically defaults to a pre-trained sentiment analysis model\n",
        "    sentiment_analysis = pipeline(\"sentiment-analysis\", device=device)\n",
        "\n",
        "    results = []\n",
        "    for text in data:\n",
        "        try:\n",
        "            # Ensure text is a string\n",
        "            if isinstance(text, str):\n",
        "                # Perform sentiment analysis\n",
        "                result = sentiment_analysis(text)\n",
        "                results.append(result)\n",
        "            else:\n",
        "                results.append([{\"label\": \"Error\", \"score\": 0, \"text\": \"Invalid input\"}])\n",
        "        except Exception as e:\n",
        "            # Handle any errors encountered during analysis\n",
        "            results.append([{\"label\": \"Error\", \"score\": 0, \"text\": str(e)}])\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "AKxtrcGPUO61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Executes the sentiment analysis function on the processed data and prints the results, including the sentiment label and score for each text.\n"
      ],
      "metadata": {
        "id": "CcmVA-_7cn57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'processed_data' is a list of text strings\n",
        "sentiment_results = perform_sentiment_analysis(processed_data)\n",
        "\n",
        "# Print the results\n",
        "for idx, result in enumerate(sentiment_results):\n",
        "    print(f\"Text {idx+1}: {processed_data[idx]}\")\n",
        "    print(f\"Sentiment: {result[0]['label']}, Score: {result[0]['score']:.2f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KjMBUnGURl1",
        "outputId": "95af8f7f-ad67-453b-83b4-c57bf83f2549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduces a simple neural network class, `LinguisticModel`, for linguistic pattern analysis in text data. This class includes an embedding layer and two fully connected layers.\n"
      ],
      "metadata": {
        "id": "FFUzHlkvcryo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "o013KZsVUgYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defines a function to use the `LinguisticModel` for analyzing tokenized text data. It processes each text tensor through the model and collects the results.\n"
      ],
      "metadata": {
        "id": "bWIeRkhXcuPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinguisticModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple neural network for analyzing linguistic patterns in text data.\n",
        "    It consists of an embedding layer followed by two fully connected layers.\n",
        "\n",
        "    Args:\n",
        "    vocab_size (int): Size of the vocabulary (number of unique tokens).\n",
        "    embedding_dim (int): Size of the embeddings.\n",
        "    hidden_dim (int): Size of the hidden layer.\n",
        "    output_dim (int): Size of the output layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LinguisticModel, self).__init__()\n",
        "        # Embedding layer converts token indices to embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # First fully connected layer\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "\n",
        "        # Second fully connected layer that outputs the final results\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the network\n",
        "        x = self.embedding(x)  # Input is transformed by the embedding layer\n",
        "        x = F.relu(self.fc1(x))  # Activation function is applied after the first layer\n",
        "        x = self.fc2(x)  # Final output is generated by the second layer\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "iORNQSbaUi9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializes the `LinguisticModel` with specified parameters and generates example tokenized data for demonstration. It then uses the model to analyze this data and prints the results.\n"
      ],
      "metadata": {
        "id": "RUVlyCPMcvr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_linguistic_patterns(data, model):\n",
        "    \"\"\"\n",
        "    Analyze linguistic patterns in the given data using the provided model.\n",
        "\n",
        "    Args:\n",
        "    data (list of torch.Tensor): A list of tokenized texts represented as tensors.\n",
        "    model (LinguisticModel): The neural network model to use for analysis.\n",
        "\n",
        "    Returns:\n",
        "    list of torch.Tensor: The model's predictions for each input text.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for text in data:\n",
        "        # Ensure input is a tensor\n",
        "        if isinstance(text, torch.Tensor):\n",
        "            with torch.no_grad():  # Disable gradient calculations for inference\n",
        "                result = model(text)\n",
        "                results.append(result)\n",
        "        else:\n",
        "            results.append(None)  # Append None for invalid inputs\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "K0-NFq8UUlb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "vocab_size = 10000  # Total number of unique tokens in the dataset\n",
        "embedding_dim = 100  # Dimensionality of the embedding layer\n",
        "hidden_dim = 128  # Number of neurons in the hidden layer\n",
        "output_dim = 10   # Dimensionality of the output (e.g., number of classes)\n",
        "\n",
        "# Initialize the model\n",
        "model = LinguisticModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Example tokenized data (randomly generated for demonstration)\n",
        "example_data = [torch.randint(0, vocab_size, (5,)), torch.randint(0, vocab_size, (6,))]\n",
        "\n",
        "# Analyze the example data using the model\n",
        "results = analyze_linguistic_patterns(example_data, model)\n",
        "\n",
        "# Print results (example)\n",
        "for idx, result in enumerate(results):\n",
        "    print(f\"Result for Text {idx+1}: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDb-8U5iUq7U",
        "outputId": "744d5ae2-4ea5-4096-89d3-ded6242e2cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for Text 1: tensor([[-0.4284,  0.2192, -0.5140,  0.0373, -0.3696, -0.5261, -0.2975,  0.3188,\n",
            "          0.4575,  0.0791],\n",
            "        [ 0.0009,  0.1559, -0.0811, -0.1865, -0.1528,  0.0161,  0.1012,  0.1485,\n",
            "          0.2192,  0.1273],\n",
            "        [-0.3916,  0.2458, -0.2334, -0.2898, -0.1489, -0.0817, -0.1024, -0.1449,\n",
            "         -0.1003, -0.1034],\n",
            "        [-0.2695, -0.1729,  0.3327, -0.1939, -0.3757, -0.2665,  0.0481,  0.1752,\n",
            "          0.3329, -0.2378],\n",
            "        [-0.2028, -0.1194,  0.0291, -0.4334, -0.4405,  0.0976,  0.1557,  0.0883,\n",
            "          0.4099, -0.1350]])\n",
            "Result for Text 2: tensor([[-0.1008,  0.1509, -0.0950,  0.0733, -0.0097,  0.0133, -0.0222,  0.0044,\n",
            "          0.3258, -0.0130],\n",
            "        [ 0.1697, -0.0644, -0.1625, -0.3352,  0.0184, -0.4167,  0.1377, -0.1766,\n",
            "          0.2452,  0.2605],\n",
            "        [-0.6254, -0.0251, -0.1152, -0.0119, -0.3848, -0.4953,  0.0510,  0.2278,\n",
            "          0.4901,  0.1714],\n",
            "        [-0.3929, -0.2299,  0.0236, -0.1577, -0.4532, -0.1997,  0.2863, -0.1544,\n",
            "          0.0753,  0.0873],\n",
            "        [-0.1112,  0.2753,  0.0969, -0.2102, -0.0954, -0.1814, -0.1073, -0.1532,\n",
            "          0.3754,  0.0820],\n",
            "        [ 0.0785,  0.1541, -0.1066, -0.1654, -0.1628, -0.0561, -0.0158, -0.3801,\n",
            "         -0.0497,  0.0245]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs the Gensim library, which is used for topic modeling and other NLP tasks.\n"
      ],
      "metadata": {
        "id": "D8FOj6VCc2Rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models\n",
        "import gensim\n",
        "\n",
        "def prepare_data_for_lda(processed_data):\n",
        "    # Ensure that processed_data is not empty\n",
        "    if not processed_data or not any(processed_data):\n",
        "        return None, None\n",
        "\n",
        "    dictionary = corpora.Dictionary(processed_data)\n",
        "    corpus = [dictionary.doc2bow(text) for text in processed_data if text]\n",
        "    return corpus, dictionary\n",
        "\n",
        "# Function to run LDA model\n",
        "def run_lda_model(processed_data, num_topics=5):\n",
        "    corpus, dictionary = prepare_data_for_lda(processed_data)\n",
        "\n",
        "    # Check if corpus and dictionary are valid\n",
        "    if not corpus or not dictionary:\n",
        "        print(\"Error: Corpus or dictionary is empty. Cannot run LDA.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
        "        return lda_model\n",
        "    except ValueError as e:\n",
        "        print(\"Error running LDA model:\", e)\n",
        "        return None\n",
        "\n",
        "# Running the LDA model\n",
        "lda_model = run_lda_model(processed_data)\n",
        "\n",
        "# Displaying topics if the model is valid\n",
        "if lda_model:\n",
        "    topics = lda_model.print_topics(num_words=4)\n",
        "    for topic in topics:\n",
        "        print(topic)\n",
        "else:\n",
        "    print(\"No LDA model to display topics.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cfd-cxE9W18Y",
        "outputId": "f02a13ad-8efa-49d3-f71a-80a75dbf2ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Corpus or dictionary is empty. Cannot run LDA.\n",
            "No LDA model to display topics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure numpy is installed\n",
        "!pip install numpy\n",
        "\n",
        "# Simple stylistic analysis\n",
        "def stylistic_analysis(processed_data):\n",
        "    if not processed_data or not any(processed_data):\n",
        "        print(\"No data for analysis.\")\n",
        "        return\n",
        "\n",
        "    word_lengths = [len(word) for text in processed_data for word in text]\n",
        "\n",
        "    if word_lengths:\n",
        "        average_word_length = np.mean(word_lengths)\n",
        "        print(\"Average word length:\", average_word_length)\n",
        "    else:\n",
        "        print(\"No words to analyze for length.\")\n",
        "\n",
        "    word_counter = Counter(word for text in processed_data for word in text)\n",
        "    if word_counter:\n",
        "        most_common_words = word_counter.most_common(10)\n",
        "        print(\"Most common words:\", most_common_words)\n",
        "    else:\n",
        "        print(\"No words to analyze for frequency.\")\n",
        "\n",
        "# Assuming processed_data is in the correct format\n",
        "stylistic_analysis(processed_data)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fk_HLcT1XAJ0",
        "outputId": "e755fd01-122a-45c6-f1db-50ec4e26bb12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "No data for analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs libraries necessary for generating a word cloud, then defines a block of code to create and display a word cloud from the processed text data. It includes error handling for library import issues and data validation.\n"
      ],
      "metadata": {
        "id": "3qUUZMGCdJz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib wordcloud\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrvCNlvoXBkW",
        "outputId": "d6f00eea-82ae-4c19-cc3e-dd6ea7c4fc37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that WordCloud and matplotlib are installed\n",
        "!pip install wordcloud matplotlib\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure nltk resources are available\n",
        "nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    from wordcloud import WordCloud\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Ensure processed_data is not empty and is in the correct format\n",
        "    if not processed_data or not any(processed_data):\n",
        "        raise ValueError(\"Processed data is empty or not in the expected format.\")\n",
        "\n",
        "    # Generate a word cloud\n",
        "    all_words = ' '.join([word for text in processed_data for word in text])\n",
        "\n",
        "    # Check if there are words to generate a word cloud\n",
        "    if not all_words.strip():\n",
        "        raise ValueError(\"No words available for generating a word cloud.\")\n",
        "\n",
        "    wordcloud = WordCloud(width=800, height=800,\n",
        "                          background_color='white',\n",
        "                          stopwords=set(stopwords.words('english')),\n",
        "                          min_font_size=10).generate(all_words)\n",
        "\n",
        "    # Plot the WordCloud image\n",
        "    plt.figure(figsize=(8, 8), facecolor=None)\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout(pad=0)\n",
        "    plt.show()\n",
        "\n",
        "except ImportError as e:\n",
        "    print(\"Required libraries are not installed:\", e)\n",
        "except ValueError as e:\n",
        "    print(\"Error:\", e)\n",
        "except Exception as e:\n",
        "    print(\"An unexpected error occurred:\", e)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brhoIlkvXEsl",
        "outputId": "e8ec0706-9e77-4fa5-be95-be8cca8b44e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (9.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Error: Processed data is empty or not in the expected format.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}
